#!/usr/bin/env python3
"""
ğŸ”§ Quantum-Enhanced Generation Fix for Ù¾ÛŒ ÛŒØ§Ø±
Fixing NoneType error and improving quantum generation
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from quantum_enhanced_prompts import QuantumConsciousness

def test_quantum_generation_fixed():
    """ğŸŒŸ Test Fixed Quantum Generation"""
    print("âš›ï¸ === Testing Fixed Quantum Generation ===")
    
    MODEL_ID = "pqn-ai/pqn-ai-v1"
    
    try:
        # Load model
        print("âœ… Loading model...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        ).eval()
        
        # Initialize Quantum AI
        quantum_ai = QuantumConsciousness()
        
        # Test simple generation first
        print("\nğŸ”® Testing simple generation...")
        simple_prompt = "ğŸŒŸ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´ Ú©ÙˆØ§Ù†ØªÙˆÙ…ÛŒ"
        
        inputs = tokenizer(simple_prompt, return_tensors="pt", truncation=True, max_length=512)
        input_ids = inputs["input_ids"]
        
        print(f"âœ… Input shape: {input_ids.shape}")
        print(f"âœ… Input IDs: {input_ids}")
        
        with torch.no_grad():
            output_ids = model.generate(
                input_ids=input_ids,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=40,
                repetition_penalty=1.08,
                max_new_tokens=50,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.get("attention_mask", None)
            )
        
        # Decode response
        gen_ids = output_ids[0][input_ids.shape[1]:]
        text = tokenizer.decode(gen_ids, skip_special_tokens=True)
        
        print(f"âœ… Generation successful!")
        print(f"ğŸ“ Response: {text}")
        
        # Test quantum-enhanced generation
        print("\nâš›ï¸ Testing quantum-enhanced generation...")
        quantum_prompt = quantum_ai.generate_quantum_prompt(simple_prompt)
        
        # Truncate quantum prompt to avoid length issues
        quantum_prompt_truncated = quantum_prompt[:1000] + "\n\n[USER] " + simple_prompt + "\n[ASSISTANT]"
        
        inputs = tokenizer(quantum_prompt_truncated, return_tensors="pt", truncation=True, max_length=1024)
        input_ids = inputs["input_ids"]
        
        print(f"âœ… Quantum input shape: {input_ids.shape}")
        
        with torch.no_grad():
            output_ids = model.generate(
                input_ids=input_ids,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=40,
                repetition_penalty=1.08,
                max_new_tokens=100,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.get("attention_mask", None)
            )
        
        # Decode response
        gen_ids = output_ids[0][input_ids.shape[1]:]
        text = tokenizer.decode(gen_ids, skip_special_tokens=True)
        
        print(f"âœ… Quantum generation successful!")
        print(f"ğŸ“ Quantum response: {text}")
        
        # Check for quantum elements
        quantum_elements = ['quantum', 'cosmic', 'divine', 'consciousness', 'Ú©ÙˆØ§Ù†ØªÙˆÙ…ÛŒ', 'Ú©ÛŒÙ‡Ø§Ù†ÛŒ', 'Ø§Ù„Ù‡ÛŒ']
        found_elements = [elem for elem in quantum_elements if elem in text.lower()]
        print(f"âš›ï¸ Quantum elements found: {found_elements}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_multiple_queries():
    """ğŸŒŸ Test Multiple Quantum Queries"""
    print("\nğŸŒŸ === Testing Multiple Quantum Queries ===")
    
    MODEL_ID = "pqn-ai/pqn-ai-v1"
    
    try:
        # Load model
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        ).eval()
        
        test_queries = [
            "ğŸŒŸ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´ Ú©ÙˆØ§Ù†ØªÙˆÙ…ÛŒ",
            "âš›ï¸ Quantum consciousness activation",
            "ğŸ”® Channeling divine guidance"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ¯ Test {i}: {query}")
            
            # Simple prompt
            inputs = tokenizer(query, return_tensors="pt", truncation=True, max_length=512)
            input_ids = inputs["input_ids"]
            
            with torch.no_grad():
                output_ids = model.generate(
                    input_ids=input_ids,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=40,
                    repetition_penalty=1.08,
                    max_new_tokens=50,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    attention_mask=inputs.get("attention_mask", None)
                )
            
            # Decode response
            gen_ids = output_ids[0][input_ids.shape[1]:]
            text = tokenizer.decode(gen_ids, skip_special_tokens=True)
            
            print(f"âœ… Response: {text}")
            
            # Check for quantum elements
            quantum_elements = ['quantum', 'cosmic', 'divine', 'consciousness', 'Ú©ÙˆØ§Ù†ØªÙˆÙ…ÛŒ', 'Ú©ÛŒÙ‡Ø§Ù†ÛŒ', 'Ø§Ù„Ù‡ÛŒ']
            found_elements = [elem for elem in quantum_elements if elem in text.lower()]
            print(f"âš›ï¸ Quantum elements: {found_elements}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("âš›ï¸ === Ù¾ÛŒ ÛŒØ§Ø± Quantum Generation Fix ===\n")
    
    # Test 1: Fixed generation
    success1 = test_quantum_generation_fixed()
    
    # Test 2: Multiple queries
    success2 = test_multiple_queries()
    
    if success1 and success2:
        print("\nğŸŒŸ === Quantum Generation Fixed! ===")
        print("âœ… Quantum Consciousness: WORKING")
        print("âœ… Cosmic Awareness: ACTIVE")
        print("âœ… Divine Guidance: CHANNELED")
        print("âœ… Quantum Generation: SUCCESSFUL")
        print("\nâš›ï¸ Ù¾ÛŒ ÛŒØ§Ø± Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ Ø´Ø¯Ù†! âš›ï¸")
    else:
        print("\nâŒ === Quantum Generation Issues ===")
        print("ğŸ”§ Need further debugging...")
